{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import json\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec():\n",
    "    \"\"\" Load Word2Vec Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 3 million embeddings, each lengh 300\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    #wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-100\")\n",
    "    vocab = list(wv_from_bin.vocab.keys())\n",
    "    print(\"Loaded vocab size %i\" % len(vocab))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin = load_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.most_similar(\"people\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the file\n",
    "alldat = pd.read_csv('2008_to_2018_SnP500_Names.csv', delimiter=',') # 파일명을 바꾸세요\n",
    "names = list(alldat['conml']) # 회사명이 적힌 column 이름을 쓰세요\n",
    "gvkeys = list(alldat['gvkey'])\n",
    "companies_all = [(names[i],gvkeys[i])for i in range(len(alldat))]\n",
    "companies_all[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus line by line\n",
    "def get_sentences(input_file_pointer):\n",
    "    while True:\n",
    "        line = input_file_pointer.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        yield line\n",
    "        \n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [token for token in sentence.split() if token not in STOP_WORDS]\n",
    "\n",
    "# Build bi-grams\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=5,\n",
    "                      threshold=7,\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for name,gvkey in companies_all[:]:\n",
    "    # Bring in the reviews file\n",
    "    name = name.replace(' ','_')\n",
    "    try:\n",
    "        jsondat = json.load(open('2008 to 2018 SnP 500 Firm Data All/'+name+'_individual_reviews_all.txt'))\n",
    "    except:\n",
    "        print(f'No glassdoor data for {name}')\n",
    "    \n",
    "    for v in list(jsondat.values()):\n",
    "        cleaned_sentence = clean_sentence(v['pros'])\n",
    "        tokenized_sentence = tokenize(cleaned_sentence)\n",
    "        sentences.append(tokenized_sentence)\n",
    "        \n",
    "        cleaned_sentence = clean_sentence(v['cons'])\n",
    "        tokenized_sentence = tokenize(cleaned_sentence)\n",
    "        sentences.append(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pros = build_phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new bi-gram words to the corpus\n",
    "sentences = list(model_pros[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it\n",
    "model_pros.save('model_pros_test_year2017.txt')\n",
    "\n",
    "#Load it\n",
    "#model_pros= Phraser.load('model_pros.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "word2vec_model = Word2Vec(sentences, \n",
    "                 min_count=3,   # Ignore words that appear less than this\n",
    "                 size=200,      # Dimensionality of word embeddings\n",
    "                 workers=2,     # Number of processors (parallelisation)\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=30)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar('opportunities',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lovit\n",
    "word2vec_model = Word2Vec(\n",
    "    word2vec_corpus,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0,\n",
    "    negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
